import torch
from transformers import AutoModelForCausalLM

#from .deepseek_vl2.models import VLChatProcessor, MultiModalityCausalLM
from deepseek_vl2.utils.io import load_pil_images
from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM


# specify the path to the model
model_path = "deepseek-ai/deepseek-vl2-tiny"
vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)
tokenizer = vl_chat_processor.tokenizer

vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

## multiple images (or in-context learning) conversation example


conversation = [  
                        {
                            "role": "<|User|>",
                            "content": f"<image>This image was generated by the Stable Diffusion, using the prompt: She is young. Can you refine the prompt, so the image can even better adhere the conditioning prompt?",
                            "images": ["/storage/outputs/tokenflow-results_cfg_scale_4.5/explicit/interventions/age/1MO3gP8vxoE_3_0/She is young./img_ode/00000.png"],
                        },
                        {"role": "<|Assistant|>", "content": ""},
                ]


conversation = [  
    {
        "role": "<|User|>",
        "content": (
            "<image>\n"
            "You are a prompt optimization assistant for Stable Diffusion.\n\n"
            "This image was generated from the prompt: **She is young**.\n"
            "Please analyze the image and determine how well it aligns with the prompt. Then, improve the current prompt: She is young  so that it better captures the intended concept(young woman).\n\n"
            "Your response should include:\n"
            "1. A short analysis of how the image deviates from the prompt.\n"
            "2. A refined prompt in the typical Stable Diffusion style (comma-separated, descriptive).\n"
            "3. An optional negative prompt to help exclude undesired features.\n"
            "4. A short rationale for your changes."
        ),
        "images": [
            "/storage/outputs/tokenflow-results_cfg_scale_4.5/explicit/interventions/age/1MO3gP8vxoE_3_0/She is young./img_ode/00000.png"
        ],
    },
    {
        "role": "<|Assistant|>",
        "content": ""
    },
]



pil_images = load_pil_images(conversation)
prepare_inputs = vl_chat_processor(
    conversations=conversation,
    images=pil_images,
    force_batchify=True,
    system_prompt=""
).to(vl_gpt.device)

# run image encoder to get the image embeddings
inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# run the model to get the response
outputs = vl_gpt.language.generate(
    inputs_embeds=inputs_embeds,
    attention_mask=prepare_inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    max_new_tokens=10000,
    do_sample=False,
    use_cache=True
)

answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=False)
print(f"{prepare_inputs['sft_format'][0]}", answer)