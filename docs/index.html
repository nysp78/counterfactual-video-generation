<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Causally Steered Diffusion for Automated Video Counterfactual Generation</title>
 <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Causally Steered Diffusion for Automated Video Counterfactual Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
  
              <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/nikos-spyrou" target="_blank">Nikos Spyrou</a><sup>1, 2, 3</sup>,
              </span>

              <span class="author-block">
                <a href="https://thanosvlo.github.io/" target="_blank"> Athanasios Vlontzos</a><sup>4</sup>,
              </span>

              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LVACmX4AAAAJ&hl=en" target="_blank">Paraskevas Pegios</a><sup>6</sup>,
              </span>

               <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/thomas-melistas" target="_blank">Thomas Melistas</a><sup>1, 2, 3</sup>,
              </span>

              <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/nefeli-gkouti" target="_blank">Nefeli Gkouti</a><sup>1, 2, 3</sup>,
              </span>
            
              <span class="author-block">
                <a href="https://scholar.google.co.uk/citations?user=z1bkjU8AAAAJ" target="_blank"> Yannis Panagakis</a><sup>1, 2</sup>,
              </span>

              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=VPsjPNUAAAAJ" target="_blank">Giorgos Papanastasiou<sup>2, 5</sup></a>,
              </span>

              <span class="author-block">
                <a href="https://vios.science/team/tsaftaris" target="_blank">Sotirios A. Tsaftaris<sup>2, 3</sup></a>
              </span>

              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>National & Kapodistrian University of Athens, Greece,
                      <sup>2</sup>Archimedes/Athena RC, Greece<br>
                      <sup>3</sup>The University of Edinburgh, UK,
                      <sup>4</sup>Imperial College London, UK<br>
                      <sup>5</sup>The University of Essex, UK,
                      <sup>6</sup>Technical University of Denmark<br>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                     <!-- ArXiv abstract Link -->
                     <span class="link-block">
                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                     <span>arXiv</span>
                  

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/nysp78/counterfactual-video-generation" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic “what-if” video scenarios in diverse areas such as  healthcare and digital media.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Method -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <h2 class="title is-3 has-text-centered">Method</h2>

            <p>
              We approach video editing as the generation of video counterfactuals, 
              where the goal is to modify specific attributes of a factual (source) video 
              (e.g., transforming a woman into a man with a beard). To generate plausible and semantically meaningful video counterfactuals, 
              we introduce a novel framework that integrates an assumed prior causal graph with a vision-language model (VLM) loss 
              using textual differentiation <a href="https://arxiv.org/abs/2406.07496" target="_blank" rel="noopener noreferrer">(Yuksekgonul et al.)</a> 
              for optimized prompt-driven causal steering. 
              Both the VLM and the LDM are treated as black boxes, 
              allowing us to focus on their interaction without the need for explicit manipulation or 
              particular knowledge of their internal workings. 
              Figure <a href="#fig2">2</a> depicts how counterfactual estimations improve with a 
              causally consistent prompt using text differentiation optimization <a href="https://arxiv.org/abs/2406.07496" target="_blank" rel="noopener noreferrer">(Yuksekgonul et al.)</a>.
            </p>

          <figure id="fig1" style="text-align: center;">
               <img 
                   src="static/images/figure2_v7.png" 
                   class="blend-img-background smaller-image" 
                   alt="VLM method diagram" 
                  style="margin-bottom: 1rem; max-width: 100%; height: auto;"
           />
           <figcaption style="font-size: 0.95em; text-align: justify;">
            \(\textbf{VLM causal steering at a glance:}\) The video editing system operates as a black-box (frozen) counterfactual generator and the (black-box) VLM as an evaluator of the generated counterfactuals. The VLM receives as input a generated counterfactual frame, the evaluation instruction, and the target counterfactual prompt \(\mathcal{P}\), and returns textual feedback, which is used to compute a “textual gradient” \(\frac{\partial \mathcal{L}}{\partial \mathcal{P}}\) and optimize \(\mathcal{P}\).
          </figcaption>
          </figure>

            <p>
            The proposed framework is illustrated in <a href="#fig1">1</a>.
            Given a generated counterfactual video frame, the counterfactual prompt, and an evaluation instruction containing the target interventions, 
            we implement our proposed “multimodal loss” using a VLM:
            </p>

            <div style="text-align: center; margin: 1em 0;">
              \[
                \mathcal{L} = \textit{VLM}(\mathcal{V'}_{\mathit{frame}}, \textit{evaluation instruction}, \mathcal{P})
              \]
          </div>


          <p>
          To optimize \(\mathcal{P}\), we employ <em>Textual Gradient Descent</em> (TGD) , which directly updates the prompt:
          </p>

    <div style="text-align: center; margin: 1em 0;">
              \[
                \begin{aligned}
                \mathcal{P}' &= \text{TGD.step} \left(\mathcal{P}, \frac{\partial \mathcal{L}}{\partial \mathcal{P}} \right) \\
                &\triangleq \textit{LLM}\Big( \textit{Below are the criticisms on } \{ \mathcal{P} \}: 
                \left\{ \frac{\partial \mathcal{L}}{\partial \mathcal{P}} \right\} \\
                &\quad \textit{Incorporate the criticisms, and produce a new prompt.} \Big)
               \end{aligned}
              \]
    </div>

    <p>
      where \(\frac{\partial \mathcal{L}}{\partial \mathcal{P}}\) denotes the “textual gradients,” passed through an LLM at each TGD update to generate a new prompt incorporating the VLM criticisms.
    </p>



          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel --> 
<section class="hero is-small" id="fig2">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item has-text-centered">
          <img 
            src="static/images/fig1/1F5naBzNfi8_0_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            First image description.
          </h2>
        </div>


          <div class="item has-text-centered">
          <img 
            src="static/images/fig1/aGRVuZHstlU_0_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            First image description.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section has-background-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <h2 class="title is-4 has-text-centered">Key Contributions</h2>

            <p>
              The main focus of this work is to automate the creation of causally faithful, well-structured, and model-aligned textual prompts, steering the counterfactual transformations toward accurate and semantically meaningful OOD edits at inference. 
              In summary, our contributions are: 
            </p>
            <ol>
              <li>We present a novel framework that allows steering diffusion-based video editing towards causal counterfactuals by propagating textual feedback from a VLM-based counterfactual loss through the LDM input prompt.</li>
              <li> We improve the causal effectiveness of counterfactual estimations by tuning the input prompt, without requiring access to LDM internal mechanisms, while preserving video quality, minimality, and temporal consistency.</li>
              <li> We demonstrate that causally faithful steering enables causally faithful counterfactual generation from LDM latent spaces.</li>
              <li>We design VLM-based evaluation metrics to further assess the capacity of diffusion-based video editing frameworks for plausible counterfactual generation.</li>
            </ol>
            <br>
           

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small colorful-section" id="causal-steering-results">
  <div class="container">
    <h1 class="title is-2">VLM Causal Steering Results</h1>

    <div class="columns is-centered is-variable is-4 mt-5">
      
    
    </div>
  </div>
</section>


<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item item-video1 tooltip">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/images/1F5naBzNfi8_0_0.gif" type="video/mp4">
          </video>
        </div>

        <div class="item item-video2 tooltip">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>



      </div>
    </div>
  </div>
</section>
-->





<!-- Paper poster --
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
