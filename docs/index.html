<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Causally Steered Diffusion for Automated Video Counterfactual Generation</title>
 <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Causally Steered Diffusion for Automated Video Counterfactual Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
  
              <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/nikos-spyrou" target="_blank">Nikos Spyrou</a><sup>1, 2, 3</sup>,
              </span>

              <span class="author-block">
                <a href="https://thanosvlo.github.io/" target="_blank"> Athanasios Vlontzos</a><sup>7</sup>,
              </span>

              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LVACmX4AAAAJ&hl=en" target="_blank">Paraskevas Pegios</a><sup>4, 5</sup>,
              </span>

               <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/thomas-melistas" target="_blank">Thomas Melistas</a><sup>1, 2, 3</sup>,
              </span>

              <span class="author-block">
                <a href="https://archimedesai.gr/en/researchers/nefeli-gkouti" target="_blank">Nefeli Gkouti</a><sup>1, 2, 3</sup>,
              </span>
            
              <span class="author-block">
                <a href="https://scholar.google.co.uk/citations?user=z1bkjU8AAAAJ" target="_blank"> Yannis Panagakis</a><sup>1, 2</sup>,
              </span>

              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=VPsjPNUAAAAJ" target="_blank">Giorgos Papanastasiou<sup>2, 6</sup></a>,
              </span>

              <span class="author-block">
                <a href="https://vios.science/team/tsaftaris" target="_blank">Sotirios A. Tsaftaris<sup>2, 3</sup></a>
              </span>

              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>National & Kapodistrian University of Athens, Greece,
                      <sup>2</sup>Archimedes/Athena RC, Greece<br>
                      <sup>3</sup>The University of Edinburgh, UK,
                      <sup>4</sup>Technical University of Denmark<br>
                      <sup>5</sup>Pioneer Centre for AI, Denmark<br>
                      <sup>6</sup>The University of Essex, UK,
                      <sup>7</sup>Monzo Bank, UK<br>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                     <!-- ArXiv abstract Link -->
                     <span class="link-block">
                        <a href="https://www.arxiv.org/abs/2506.14404" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                     <span>arXiv</span>
                  

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/nysp78/counterfactual-video-generation" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Adapting text-to-image (T2I) latent diffusion models (LDMs) to video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships inherent to the video data generating process. Edits affecting causally dependent attributes often generate unrealistic or misleading outcomes if these relationships are ignored. In this work, we introduce a causally faithful framework for counterfactual video generation, formulated as an Out-of-Distribution (OOD) prediction problem. We embed prior causal knowledge by encoding the relationships specified in a causal graph into text prompts and guide the generation process by optimizing these prompts using a vision-language model (VLM)-based textual loss. This loss encourages the latent space of the LDMs to capture OOD variations in the form of counterfactuals, effectively steering generation toward causally meaningful alternatives. The proposed framework, dubbed CSVC, is agnostic to the underlying video editing system and does not require access to its internal mechanisms or fine-tuning. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Experimental results show that CSVC generates causally faithful video counterfactuals within the LDM distribution via prompt-based causal steering, achieving state-of-the-art causal effectiveness without compromising temporal consistency or visual quality on real-world facial videos. Due to its compatibility with any black-box video editing system, our framework has significant potential to generate realistic 'what if' hypothetical video scenarios in diverse areas such as digital media and healthcare.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel --> 
<section class="hero is-small" id="fig1">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item has-text-centered">
          <img 
            src="static/images/fig1/1F5naBzNfi8_0_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age: Transforming an old woman to young.
          </h2>
        </div>


          <div class="item has-text-centered">
          <img 
            src="static/images/fig1/aGRVuZHstlU_0_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on gender: Tranforming a woman into a man with a beard.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <h2 class="title is-3 has-text-centered">Method</h2>

            <p>
              We approach video editing as the generation of video counterfactuals, 
              where the goal is to modify specific attributes of a factual (source) video 
              (e.g., transforming a woman into a man with a beard). To generate plausible and semantically meaningful video counterfactuals, 
              we introduce a novel framework that integrates an assumed prior causal graph with a vision-language model (VLM) loss 
              using textual differentiation <a href="https://arxiv.org/abs/2406.07496" target="_blank" rel="noopener noreferrer">(Yuksekgonul et al.)</a> 
              for optimized prompt-driven causal steering. 
              Both the VLM and the LDM are treated as black boxes, 
              allowing us to focus on their interaction without the need for explicit manipulation or 
              particular knowledge of their internal workings. 
              Figure <a href="#fig1">1</a> depicts how counterfactual estimations improve with a 
              causally consistent prompt using text differentiation optimization.
            </p>

            <figure id="fig2" style="text-align: center;">
              <img 
                src="static/images/figure2_v8.png" 
                class="blend-img-background smaller-image" 
                alt="VLM method diagram" 
                style="margin-bottom: 1rem; max-width: 100%; height: auto;"
              />
              <figcaption style="font-size: 0.95em; text-align: justify;">
                \(\textbf{CSVC at a glance:}\)  The initial counterfactual prompts (e.g., <em>She is young</em>) are generated using GPT-4 
                by providing the causal graph and the factual prompts (e.g., <em>She is old</em>) and leveraging 
                in-context learning</a>. 
                The video editing system operates as a black-box (frozen) counterfactual generator, 
                and the (black-box) VLM as an evaluator of the generated counterfactuals. 
                The VLM takes as input a generated counterfactual frame, the evaluation instruction, 
                and the target counterfactual prompt \( \mathcal{P} \), and outputs textual feedback 
                used to compute a ‘textual gradient’ 
                \( \frac{\partial \mathcal{L}}{\partial \mathcal{P}} \), 
                which guides the optimization of \( \mathcal{P} \) by focusing on the unsuccessful interventions.
              </figcaption>
            </figure>

            <p>
              The proposed framework is illustrated in <a href="#fig2">2</a>.
              Given a generated counterfactual video frame, the counterfactual prompt, and an evaluation instruction containing the target interventions, 
              we implement our proposed “multimodal loss” using a VLM:
            </p>

            <div style="text-align: center; margin: 1em 0;">
              \[
                \mathcal{L} = \textit{VLM}(\mathcal{V'}_{\mathit{frame}}, \textit{evaluation instruction}, \mathcal{P})
              \]
            </div>

            <p>
              To optimize \(\mathcal{P}\), we employ <em>Textual Gradient Descent</em> (TGD), which directly updates the prompt:
            </p>

            <div style="text-align: center; margin: 1em 0;">
              \[
                \begin{aligned}
                \mathcal{P}' &= \text{TGD.step} \left(\mathcal{P}, \frac{\partial \mathcal{L}}{\partial \mathcal{P}} \right) \\
                &\triangleq \textit{LLM}\Big( \textit{Below are the criticisms on } \{ \mathcal{P} \}: 
                \left\{ \frac{\partial \mathcal{L}}{\partial \mathcal{P}} \right\} \\
                &\quad \textit{Incorporate the criticisms, and produce a new prompt.} \Big)
                \end{aligned}
              \]
            </div>

            <p>
              where \(\frac{\partial \mathcal{L}}{\partial \mathcal{P}}\) denotes the “textual gradients,” passed through an LLM at each TGD update to generate a new prompt incorporating the VLM criticisms.
            </p>

            <!-- Textual Gradient Carousel -->
            <style>
              .carousel-container {
                overflow: hidden;
                position: relative;
                width: 100%;
              }

              .carousel-track {
                display: flex;
                transition: transform 0.5s ease-in-out;
                width: 100%;
              }

              .carousel-slide {
                flex: 0 0 100%;
                max-width: 100%;
                box-sizing: border-box;
                padding: 1.5rem;
                display: flex;
                flex-direction: column;
                align-items: center;
              }

              .carousel-slide figure {
                width: 100%;
                max-width: 800px;
              }

              .carousel-slide pre {
                background-color: #c7e0dd;
                padding: 1rem;
                border-radius: 12px;
                white-space: pre-wrap;
                font-family: monospace;
                font-size: 0.95em;
                line-height: 1.6;
                width: 100%;
                box-sizing: border-box;
              }

              .carousel-buttons {
                margin-top: 1.5rem;
                display: flex;
                justify-content: center;
                gap: 1rem;
              }

              .carousel-buttons button {
                border: none;
                background: #f5f5f5;
                padding: 0.75rem 1rem;
                border-radius: 8px;
                cursor: pointer;
                font-size: 1.2rem;
              }

              .carousel-buttons button:hover {
                background-color: #e2e2e2;
              }
            </style>

            <div class="box" style="margin-top: 2rem;">
              <div class="carousel-container">
                <div class="carousel-track" id="gradientCarousel">
                  
                  <!-- Slide 1 -->
                  <div class="carousel-slide">
                    <figure>
                      <figcaption class="has-text-centered has-text-weight-semibold is-italic" style="margin-bottom: 1rem;">
                        Textual Gradient \(\frac{\partial \mathcal{L}}{\partial \mathcal{P}}\) — Age Intervention of Figure <a href="#fig1">1</a> (old to young)
                      </figcaption>
                      <pre>
The variable "A woman is young" lacks specificity and clarity, which likely contributed to the misalignment between the generated image and the intended attribute of youthfulness. The term "young" is highly subjective and can be interpreted variably depending on cultural, personal, and contextual factors. This ambiguity in the prompt may have led the Text-to-Image model to generate an image that does not meet the counterfactual criteria specified.
 
 **Feedback and Criticism:**
 
 1. **Specificity in Age Description**: The term "young" should be replaced with a more precise age or age range to guide the model more effectively. For instance, specifying "a woman in her early twenties" or "a woman aged 20-25 years" would provide a clearer target for the model, reducing ambiguity and increasing the likelihood of generating an image that aligns with the youthful attribute desired.
 
 2. **Inclusion of Contextual Details**: While the prompt should remain focused on the age attribute, adding minimal contextual details that align with youthful appearances could enhance the effectiveness of the prompt. For example, mentioning "a youthful demeanor" or "fresh-faced" could cue the model to focus on features typically associated with younger individuals.
 
 3. **Avoiding Overly Broad Descriptors**: The use of broad descriptors like "young" without additional qualifiers can lead to a wide range of interpretations. It's beneficial to avoid these where precision is required for generating specific outcomes.
 
 4. **Testing and Iteration**: After adjusting the prompt, it would be advisable to test the new prompt with the model to see if the changes yield a more accurate representation. This iterative approach can help fine-tune the prompt based on actual outcomes.
 
 By addressing these points, the prompt "A woman is young" can be optimized to more effectively communicate the desired attribute of youthfulness to the Text-to-Image model, thereby improving the alignment of the generated image with the counterfactual target.
                      </pre>
                    </figure>
                  </div>

                  <!-- Slide 2 -->
                  <div class="carousel-slide">
                    <figure>
                      <figcaption class="has-text-centered has-text-weight-semibold is-italic" style="margin-bottom: 1rem;">
                        Textual Gradient \(\frac{\partial \mathcal{L}}{\partial \mathcal{P}}\) — Gender Intervention of Age Intervention of Figure <a href="#fig2">2</a> (woman to man)
                      </figcaption>
                      <pre>
The variable "He is young, he has a beard" needs to be optimized to better align with the specified attributes of being a young man with a beard. The current prompt lacks specificity and clarity, which may have contributed to the generation of an image that does not meet the counterfactual target attributes.
 
 **Feedback and Criticism:**
 
 1. **Specificity and Clarity**: The current prompt is somewhat vague and does not explicitly mention the gender, which is crucial for the intervention. The phrase "he has a beard" implies masculinity, but it seems that the model did not pick up on this cue effectively. To improve, the prompt should explicitly state the gender to avoid ambiguity. For example, incorporating the word "man" or "male" could guide the model more effectively.
 
 2. **Descriptive Language**: The prompt could benefit from more descriptive language to emphasize the attributes. Instead of just saying "young," it might be helpful to describe what "young" typically implies in this context, such as "a youthful appearance" or "looks in his twenties." This could help the model in generating features that are commonly associated with younger individuals.
 
 3. **Attribute Focus**: The prompt should focus more on the attributes that need intervention. Since "beard" is a key attribute but was completely missed, the prompt could emphasize this feature more strongly. For instance, describing the beard in more detail like "sporting a well-groomed beard" could make it a focal point for the generation process.
 
 4. **Use of Active Language**: The prompt could use more active language to make the descriptions more dynamic and engaging, which might help in better capturing the desired attributes. For example, "A young man with a striking beard" adds character and emphasis.
 
 5. **Avoiding Misinterpretation**: To ensure that the attributes are not misinterpreted, the prompt could include a brief explanation that aligns with common perceptions or stereotypes, ensuring that the text-to-image model has a clear and direct reference to work from.
 
 By addressing these points, the prompt can be optimized to more effectively communicate the desired attributes to the text-to-image model, thereby improving the likelihood of generating an image that aligns with the counterfactual target attributes.
                      </pre>
                    </figure>
                  </div>

                </div>

                <!-- Carousel navigation -->
                <div class="carousel-buttons">
                  <button onclick="moveGradientSlide(-1)">◀</button>
                  <button onclick="moveGradientSlide(1)">▶</button>
                </div>
              </div>
            </div>

            <script>
              let gradientSlideIndex = 0;

              function moveGradientSlide(direction) {
                const track = document.getElementById("gradientCarousel");
                const slides = document.querySelectorAll(".carousel-slide");
                gradientSlideIndex = (gradientSlideIndex + direction + slides.length) % slides.length;
                track.style.transform = `translateX(-${gradientSlideIndex * 100}%)`;
              }

              document.addEventListener("DOMContentLoaded", () => {
                const track = document.getElementById("gradientCarousel");
                track.style.display = "flex";
                track.style.transition = "transform 0.5s ease-in-out";
                document.querySelectorAll(".carousel-slide").forEach(slide => {
                  slide.style.minWidth = "100%";
                });
              });
            </script>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<!--
<section class="section has-background-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <h2 class="title is-4 has-text-centered">Key Contributions</h2>

            <p>
              The main focus of this work is to automate the creation of causally faithful, well-structured, and model-aligned textual prompts, steering the counterfactual transformations toward accurate and semantically meaningful OOD edits at inference. 
              In summary, our contributions are: 
            </p>
            <ol>
              <li>We present a novel framework that allows steering diffusion-based video editing towards causal counterfactuals by propagating textual feedback from a VLM-based counterfactual loss through the LDM input prompt.</li>
              <li> We improve the causal effectiveness of counterfactual estimations by tuning the input prompt, without requiring access to LDM internal mechanisms, while preserving video quality, minimality, and temporal consistency.</li>
              <li> We demonstrate that causally faithful steering enables causally faithful counterfactual generation from LDM latent spaces.</li>
              <li>We design VLM-based evaluation metrics to further assess the capacity of diffusion-based video editing frameworks for plausible counterfactual generation.</li>
            </ol>
            <br>
           

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<section class="section has-background-light" id="causal-steering-results">
  <div class="container">
    <h1 class="title is-2 has-text-centered">CSVC Results</h1>
     <h2 class="title is-4 has-text-centered">FLATTEN</h2>
     <section class="hero is-small" id="fig3">
          <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">

                      <div class="item has-text-centered">
                <img 
            src="static/images/flatten/-_zyvfId578_12_1.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on beard.
          </h2>
        </div>


        <div class="item has-text-centered">
          <img 
            src="static/images/flatten/g_Yrrk4eoXk_13_6.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age
          </h2>
        </div>

         <div class="item has-text-centered">
          <img 
            src="static/images/flatten/17RnfGHRh9E_5_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on beard
          </h2>
        </div>


         <div class="item has-text-centered">
          <img 
            src="static/images/flatten/0xtFFvocggE_6_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age
          </h2>
        </div>

      </div>
    </div>
  </div>
  </section>


     <h2 class="title is-4 has-text-centered">TokenFlow</h2>

      <section class="hero is-small" id="fig2">
          <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">


          <div class="item has-text-centered">
          <img 
            src="static/images/tokenflow/vMkIT1SycG8_14_1.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age
          </h2>
        </div>

        <div class="item has-text-centered">
                <img 
            src="static/images/tokenflow/m_YWPYjVqa4_9_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on gender
          </h2>
        </div>



         <div class="item has-text-centered">
          <img 
            src="static/images/tokenflow/-AdQECaFzxU_1_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age
          </h2>
        </div>

        
      </div>
    </div>
  </div>
  </section>



     <h2 class="title is-4 has-text-centered">Tune-A-Video</h2>

      <section class="hero is-small" id="fig2">
          <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">

                      <div class="item has-text-centered">
                <img 
            src="static/images/tuneavideo/cImOMABtLFI_2_1.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on beard
          </h2>
        </div>


          <div class="item has-text-centered">
          <img 
            src="static/images/tuneavideo/xkzBmNcfBB8_0_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on age
          </h2>
        </div>

         <div class="item has-text-centered">
          <img 
            src="static/images/tuneavideo/Zeem6k2bDFk_6_0.gif" 
            alt="MY ALT TEXT" 
            style="width: 100%; max-width: 900px; height: auto; display: inline-block;" 
          />
          <h2 class="subtitle has-text-centered">
            Intervention on beard
          </h2>
        </div>

      </div>
    </div>
  </div>
  </section>



   


    <div class="columns is-centered is-variable is-4 mt-5">

    
    </div>
  </div>

</section>


<section class="section has-background-white" id="causal-steering-results_v2">
  <div class="container">
    <h2 class="title is-4 has-text-centered">
      Progressive Counterfactual Transformation with VLM Causal Steering
    </h2>

    <div class="has-text-centered" id="fig4" style="margin-top: 1.5rem;">
      <img 
        src="static/images/flatten/1MO3gP8vxoE_3_0.gif" 
        alt="Progressive transformation GIF" 
        style="width: 100%; max-width: 900px; height: auto;"
      />
      <p class="subtitle has-text-centered mt-3" style="max-width: 800px; margin: 0 auto;">
        Progressive counterfactual transformation of an old woman to young through causal prompt steering.
      </p>
    </div>
  </div>
</section>






<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item item-video1 tooltip">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/images/1F5naBzNfi8_0_0.gif" type="video/mp4">
          </video>
        </div>

        <div class="item item-video2 tooltip">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>



      </div>
    </div>
  </div>
</section>
-->





<!-- Paper poster --
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{spyrou2025causally,
  title={Causally Steered Diffusion for Automated Video Counterfactual Generation},
  author={Spyrou, Nikos and Vlontzos, Athanasios and Pegios, Paraskevas and Melistas, Thomas and Gkouti, Nefeli and Panagakis, Yannis and Papanastasiou, Giorgos and Tsaftaris, Sotirios A},
  journal={arXiv preprint arXiv:2506.14404},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
